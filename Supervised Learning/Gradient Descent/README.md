# Gradient Descent
As observed in a previous lecture, we may train a single neurons weights and bias by iteratively applying a predefined rule. The description that we gave for the perceptron update rule was intended to hint at a more general rule for *optimizing the cost function of a model*. More specifically, we were hinting at a general continuous optimization technique called **gradient descent**. 

![alt](https://i.stack.imgur.com/recxI.png)

## Datasets
Randomly generated numberical data is used in this file.

## Packages
* matplotlib
* numpy
* seaborn
