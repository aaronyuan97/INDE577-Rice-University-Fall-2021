{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513619b5",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes.[1] Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems.\n",
    "\n",
    "In information technology (IT), an artificial neural network (ANN) is a system of hardware and/or software patterned after the operation of neurons in the human brain. ANNs -- also called, simply, neural networks -- are a variety of deep learning technology, which also falls under the umbrella of artificial intelligence, or AI.\n",
    "\n",
    "Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition.\n",
    "\n",
    "## How does a Neural Network work?\n",
    "This concept can best be understood with an example. Imagine the \"simple\" problem of trying to determine whether or not an image contains a cat. While this is rather easy for a human to figure out, it is much more difficult to train a computer to identify a cat in an image using classical methods. Considering the diverse possibilities of how a cat may look in a picture, writing code to account for every scenario is almost impossible. But using machine learning, and more specifically neural networks, the program can use a generalized approach to understanding the content in an image. Using several layers of functions to decompose the image into data points and information that a computer can use, the neural network can start to identify trends that exist across the many, many examples that it processes and classify images by their similarities. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"b920b4b3-7347-4712-b1bd-068775ec4562.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "After processing many training examples of cat images, the algorithm has a model of what elements, and their respective relationships, in an image are important to consider when deciding whether a cat is present in the picture or not. When evaluating a new image, the neural net compares the data points about the new image to its model, which is based off of all previous evaluations. It then uses some simple statistics to decides whether the image contains a cat or not based on how closely it matches the model.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"neuralnet.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c4727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) =fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506cb00",
   "metadata": {},
   "source": [
    "The data structures train_x and test_x are stored as 3 dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d42929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"np.shape(train_X) ={np.shape(train_X)}\")\n",
    "print(f\"np.shape(test_X) = {np.shape(test_X)} \\n\")\n",
    "\n",
    "print(f\"np.shape(train_X[0]) = {np.shape(train_X[0])}\")\n",
    "print(f\"np.shape(test_X[0]) = {np.shape(test_X[0])} \\n\")\n",
    "\n",
    "print(f\"train_X[0] = {train_X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349a0a2",
   "metadata": {},
   "source": [
    "Each image is comprised of a 28*28 grey scaled grid of pixel values. These values are floating point numbers in the interval （0，1）, where darker pixels will have values closer to 1 and lighter pixels will have values closer to . The following image represents one such example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_y[0] = } \\n\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d59333",
   "metadata": {},
   "source": [
    "## Image Flattening\n",
    "Simple dense neural networks pass feature vectors into the 0-th (the first) layer of the computational graph represented by the neural network structure as column vectors. This 0-th layer is essentially the same as with single neuron models. In order to feed our images into such a network we must flatten the matrix into a column vector.\n",
    "\n",
    "## Drawing\n",
    "\n",
    "We can do this for each image matrix that we are considering by calling the flatten() method together with the reshape(784, 1) method to insure it is a column vector. Note, that each image matrix is 28 by 28, and so, 784 is the number of rows in the flattened matrix column vector. The numerical values in the flattened training and testing data matrices vary between 0 and 255. These large differences in possible values can lead to problems when training the weights and the biases of the neural network. A quick and dirty fix will be to scale all data to belong in the interval (0,1) i.e., divide the entries by the largest possible value; in this case 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "\n",
    "# Flatten the training images into coloumn vectors. \n",
    "flat_train_X = []\n",
    "# One hot encode the training labels\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(train_X, train_y):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "   \n",
    "\n",
    "# Do the same for the testing data \n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(test_X, test_y):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dab37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the Network Architecture \n",
    "For our purposes, we will build a multilayered **fully connected**, or **dense**, neural network with $L$ layers, $784$ input notes, $L-2$ hidden layers of arbitrary size, and $10$ output nodes. \n",
    "\n",
    "\n",
    "For our activation function, we will use the sigmoid function:\n",
    "\n",
    "* Sigmoid Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "For our cost function, we will use the Mean Sqaure Error cost:\n",
    "$$\n",
    "C(W, b) = \\frac{1}{2}\\sum_{k=1}^{10}(\\hat{y}^{(i)}_k - y^{(i)}_k)^2.\n",
    "$$\n",
    "\n",
    "Our goal will be to write a custom Python class implementing our desired structure. However, before doing so, we first sequentually write functions to better understand the process of programming the following:\n",
    "\n",
    "* Initializing the weights and biases of each layer\n",
    "* The feedforward phase\n",
    "* Calculation of the cost function\n",
    "* Calculation of the gradient\n",
    "* Iterating stochastic gradient descent\n",
    "\n",
    "First we will define our sigmoid activation function, its derivative, and the mean squared error for a single instance of training data. Do this by running the following code. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return .5*sum((a[i] - y[i])**2 for i in range(10))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "    # The following Python lists will contain numpy matrices\n",
    "    # connected the layers in the neural network \n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        # The scalling factor is something I found in a research paper :)\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "    \n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f463116",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedforward Phase\n",
    "\n",
    "For $\\ell = 1, \\dots, L$, each layer $\\ell$ in our network will have two phases, the preactivation phase $$\\mathbf{z}^{\\ell} = W^{\\ell}\\mathbf{a}^{\\ell-1} + \\mathbf{b}^{\\ell},$$ and postactivation phase $$\\mathbf{a}^{\\ell} = \\sigma(\\mathbf{z}^{\\ell}).$$ The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through an activation function elementwise. Note $\\mathbf{a}^0 = \\mathbf{x}^{(i)}$, where $\\mathbf{x}^{(i)}$ is the current input data into our network. \n",
    "\n",
    "We can test our activation functions and matrix dimensions by running the following code which manually implements the feedforward process on a neural network with the given dimensions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()\n",
    "\n",
    "xi = flat_train_X[0]\n",
    "yi = onehot_train_y[0]\n",
    "a0 = xi\n",
    "\n",
    "print(f\"np.shape(a0) = {np.shape(a0)} \\n\")\n",
    "\n",
    "z1 = W[1] @ a0 + B[1]\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "print(f\"np.shape(W[1]) = {np.shape(W[1])}\")\n",
    "print(f\"np.shape(z1) = {np.shape(z1)}\")\n",
    "print(f\"np.shape(a1) = {np.shape(a1)} \\n\")\n",
    "\n",
    "z2 = W[2] @ a1 + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(f\"np.shape(W[2]) = {np.shape(W[2])}\")\n",
    "print(f\"np.shape(z2) = {np.shape(z2)}\")\n",
    "print(f\"np.shape(a2) = {np.shape(a2)} \\n\")\n",
    "\n",
    "z3 = W[3] @ a2 + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "y_hat = a3\n",
    "print(f\"np.shape(W[3]) = {np.shape(W[3])}\")\n",
    "print(f\"np.shape(z3) = {np.shape(z3)}\")\n",
    "print(f\"np.shape(a3) = {np.shape(a3)} \\n\")\n",
    "\n",
    "\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, B, xi, predict_vector = False):\n",
    "    Z = [[0.0]]\n",
    "    A = [xi]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "        \n",
    "    if predict_vector == False:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1]\n",
    "\n",
    "def predict(W, B, xi):\n",
    "    _, A = forward_pass(W, B, xi)\n",
    "    return np.argmax(A[-1])\n",
    "\n",
    "y_hat = forward_pass(W, B, flat_train_X[0], predict_vector=True)\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_experiment(W, B, data_features, data_labels):\n",
    "    i = np.random.randint(len(data_features))\n",
    "    print(f\"Actual label: {np.argmax(data_labels[i])}\")\n",
    "    print(f\"Predicted label: {predict(W, B, data_features[i])}\")\n",
    "    \n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    m = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        m+=1\n",
    "    return cost/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE(W, B, flat_train_X, onehot_train_y) = {MSE(W, B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "random_experiment(W, B, flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c05f10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backpropogation Phase with Stochastic Gradient Descent \n",
    "We are now ready to define a custom Python ```DenseNetwork``` class which initializes the weights and bias for the network, and implements stochastic gradient descent shown below:\n",
    "\n",
    "1. For each $i = 1, \\dots, N$.\n",
    "2. Feedforward $\\mathbf{x}^{(i)}$ into the network. \n",
    "3. Compute $\\delta^{L} = \\nabla_aC\\otimes \\sigma'(\\mathbf{z}^{L})$.\n",
    "4. For $\\ell = L-1, \\dots, 1$, compute $\\delta^{\\ell} = \\big ( (\\mathbf{w}^{\\ell + 1})^{T} \\delta^{\\ell + 1} \\Big )\\otimes \\sigma'(\\mathbf{z}^{\\ell})$.\n",
    "5. For $\\ell = L, L-1, \\dots, 1$, \n",
    "\n",
    "$$\n",
    "w^{\\ell} \\leftarrow w^{\\ell} - \\alpha \\delta^{\\ell}(\\mathbf{a}^{\\ell-1})^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{\\ell} \\leftarrow b^{\\ell} - \\alpha \\delta^{\\ell}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e717c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers = self.layers)\n",
    "\n",
    "    def train(self, X_train, y_train, alpha = 0.046, epochs = 4):\n",
    "        # Print the initial mean squared error\n",
    "        self.errors_ = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors_[0]}\")\n",
    "\n",
    "        # Find your sample size\n",
    "        sample_size = len(X_train)\n",
    "\n",
    "        # Find the number of non-input layers.\n",
    "        L = len(self.layers) - 1\n",
    "\n",
    "        # For each epoch perform stochastic gradient descent. \n",
    "        for k in range(epochs):\n",
    "            # Loop over each (xi, yi) training pair of data.\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                # Compute the output error \n",
    "                output_error = (A[L] - yi)*d_sigmoid(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function \n",
    "                # is non-inclusive. \n",
    "                for i in range(L-1, 0, -1):\n",
    "                    # Compute the node errors at each hidden layer\n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*d_sigmoid(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient \n",
    "                # descent. \n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the user the cost over all training examples\n",
    "            self.errors_.append(MSE(self.W, self.B, X_train, y_train))   \n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors_[-1]}\")\n",
    "    \n",
    "\n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network with 784 input nodes, two hidden layers with 60 nodes each \n",
    "# and a output layer with 10 nodes. \n",
    "net = DenseNetwork(layers = [784, 120, 145, 120, 10])\n",
    "\n",
    "# Check the mean squared error before training \n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "# Make a random prediction before training\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525924cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddae3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935939d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a7a1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification Error\n",
    "Let us now calculate the classification percentage on the testing data for our trained dense neural network. Recall that this is simply the number of correct labels divided by the total number of data points:\n",
    "\n",
    "This can be done by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([int(net.predict(x) == y) for x, y in zip(flat_test_X, test_y)])/len(onehot_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4604b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) =mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1076a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f428cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc040175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
