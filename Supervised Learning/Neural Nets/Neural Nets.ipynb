{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513619b5",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes.[1] Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems.\n",
    "\n",
    "In information technology (IT), an artificial neural network (ANN) is a system of hardware and/or software patterned after the operation of neurons in the human brain. ANNs -- also called, simply, neural networks -- are a variety of deep learning technology, which also falls under the umbrella of artificial intelligence, or AI.\n",
    "\n",
    "Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition.\n",
    "\n",
    "## How does a Neural Network work?\n",
    "This concept can best be understood with an example. Imagine the \"simple\" problem of trying to determine whether or not an image contains a cat. While this is rather easy for a human to figure out, it is much more difficult to train a computer to identify a cat in an image using classical methods. Considering the diverse possibilities of how a cat may look in a picture, writing code to account for every scenario is almost impossible. But using machine learning, and more specifically neural networks, the program can use a generalized approach to understanding the content in an image. Using several layers of functions to decompose the image into data points and information that a computer can use, the neural network can start to identify trends that exist across the many, many examples that it processes and classify images by their similarities. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"b920b4b3-7347-4712-b1bd-068775ec4562.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "After processing many training examples of cat images, the algorithm has a model of what elements, and their respective relationships, in an image are important to consider when deciding whether a cat is present in the picture or not. When evaluating a new image, the neural net compares the data points about the new image to its model, which is based off of all previous evaluations. It then uses some simple statistics to decides whether the image contains a cat or not based on how closely it matches the model.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"neuralnet.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86fd658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-macosx_10_14_x86_64.whl (217.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.4 MB 25.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 136.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 24.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001b[K     |████████████████████████████████| 962 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 82.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-macosx_10_10_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 87.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20\n",
      "  Downloading numpy-1.22.3-cp38-cp38-macosx_10_14_x86_64.whl (17.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.6 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 100.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 49.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 11.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 36.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 49.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 33.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 97.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 96.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 40.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=6c06c5231b79a761f9e66fd2a1eb971fd74dc97aac36fecda25418296c9e7a00\n",
      "  Stored in directory: /Users/mingzeyuan/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/Users/mingzeyuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.2.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "astroid 2.5 requires wrapt<1.13,>=1.11, but you have wrapt 1.14.1 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.6 numpy-1.22.3 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-62.1.0 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow --ignore-installed --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5e5c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ce0603bf04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfashion_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) =fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506cb00",
   "metadata": {},
   "source": [
    "The data structures train_x and test_x are stored as 3 dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d42929",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8a074b0fbf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"np.shape(train_X) ={np.shape(train_X)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"np.shape(test_X) = {np.shape(test_X)} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"np.shape(train_X[0]) = {np.shape(train_X[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"np.shape(test_X[0]) = {np.shape(test_X[0])} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"np.shape(train_X) ={np.shape(train_X)}\")\n",
    "print(f\"np.shape(test_X) = {np.shape(test_X)} \\n\")\n",
    "\n",
    "print(f\"np.shape(train_X[0]) = {np.shape(train_X[0])}\")\n",
    "print(f\"np.shape(test_X[0]) = {np.shape(test_X[0])} \\n\")\n",
    "\n",
    "print(f\"train_X[0] = {train_X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349a0a2",
   "metadata": {},
   "source": [
    "Each image is comprised of a 28*28 grey scaled grid of pixel values. These values are floating point numbers in the interval （0，1）, where darker pixels will have values closer to 1 and lighter pixels will have values closer to . The following image represents one such example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_y[0] = } \\n\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d59333",
   "metadata": {},
   "source": [
    "## Image Flattening\n",
    "Simple dense neural networks pass feature vectors into the 0-th (the first) layer of the computational graph represented by the neural network structure as column vectors. This 0-th layer is essentially the same as with single neuron models. In order to feed our images into such a network we must flatten the matrix into a column vector.\n",
    "\n",
    "## Drawing\n",
    "\n",
    "We can do this for each image matrix that we are considering by calling the flatten() method together with the reshape(784, 1) method to insure it is a column vector. Note, that each image matrix is 28 by 28, and so, 784 is the number of rows in the flattened matrix column vector. The numerical values in the flattened training and testing data matrices vary between 0 and 255. These large differences in possible values can lead to problems when training the weights and the biases of the neural network. A quick and dirty fix will be to scale all data to belong in the interval (0,1) i.e., divide the entries by the largest possible value; in this case 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "\n",
    "# Flatten the training images into coloumn vectors. \n",
    "flat_train_X = []\n",
    "# One hot encode the training labels\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(train_X, train_y):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "   \n",
    "\n",
    "# Do the same for the testing data \n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(test_X, test_y):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dab37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the Network Architecture \n",
    "For our purposes, we will build a multilayered **fully connected**, or **dense**, neural network with $L$ layers, $784$ input notes, $L-2$ hidden layers of arbitrary size, and $10$ output nodes. \n",
    "\n",
    "\n",
    "For our activation function, we will use the sigmoid function:\n",
    "\n",
    "* Sigmoid Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "For our cost function, we will use the Mean Sqaure Error cost:\n",
    "$$\n",
    "C(W, b) = \\frac{1}{2}\\sum_{k=1}^{10}(\\hat{y}^{(i)}_k - y^{(i)}_k)^2.\n",
    "$$\n",
    "\n",
    "Our goal will be to write a custom Python class implementing our desired structure. However, before doing so, we first sequentually write functions to better understand the process of programming the following:\n",
    "\n",
    "* Initializing the weights and biases of each layer\n",
    "* The feedforward phase\n",
    "* Calculation of the cost function\n",
    "* Calculation of the gradient\n",
    "* Iterating stochastic gradient descent\n",
    "\n",
    "First we will define our sigmoid activation function, its derivative, and the mean squared error for a single instance of training data. Do this by running the following code. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return .5*sum((a[i] - y[i])**2 for i in range(10))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "    # The following Python lists will contain numpy matrices\n",
    "    # connected the layers in the neural network \n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        # The scalling factor is something I found in a research paper :)\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "    \n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f463116",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedforward Phase\n",
    "\n",
    "For $\\ell = 1, \\dots, L$, each layer $\\ell$ in our network will have two phases, the preactivation phase $$\\mathbf{z}^{\\ell} = W^{\\ell}\\mathbf{a}^{\\ell-1} + \\mathbf{b}^{\\ell},$$ and postactivation phase $$\\mathbf{a}^{\\ell} = \\sigma(\\mathbf{z}^{\\ell}).$$ The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through an activation function elementwise. Note $\\mathbf{a}^0 = \\mathbf{x}^{(i)}$, where $\\mathbf{x}^{(i)}$ is the current input data into our network. \n",
    "\n",
    "We can test our activation functions and matrix dimensions by running the following code which manually implements the feedforward process on a neural network with the given dimensions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()\n",
    "\n",
    "xi = flat_train_X[0]\n",
    "yi = onehot_train_y[0]\n",
    "a0 = xi\n",
    "\n",
    "print(f\"np.shape(a0) = {np.shape(a0)} \\n\")\n",
    "\n",
    "z1 = W[1] @ a0 + B[1]\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "print(f\"np.shape(W[1]) = {np.shape(W[1])}\")\n",
    "print(f\"np.shape(z1) = {np.shape(z1)}\")\n",
    "print(f\"np.shape(a1) = {np.shape(a1)} \\n\")\n",
    "\n",
    "z2 = W[2] @ a1 + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(f\"np.shape(W[2]) = {np.shape(W[2])}\")\n",
    "print(f\"np.shape(z2) = {np.shape(z2)}\")\n",
    "print(f\"np.shape(a2) = {np.shape(a2)} \\n\")\n",
    "\n",
    "z3 = W[3] @ a2 + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "y_hat = a3\n",
    "print(f\"np.shape(W[3]) = {np.shape(W[3])}\")\n",
    "print(f\"np.shape(z3) = {np.shape(z3)}\")\n",
    "print(f\"np.shape(a3) = {np.shape(a3)} \\n\")\n",
    "\n",
    "\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, B, xi, predict_vector = False):\n",
    "    Z = [[0.0]]\n",
    "    A = [xi]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "        \n",
    "    if predict_vector == False:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1]\n",
    "\n",
    "def predict(W, B, xi):\n",
    "    _, A = forward_pass(W, B, xi)\n",
    "    return np.argmax(A[-1])\n",
    "\n",
    "y_hat = forward_pass(W, B, flat_train_X[0], predict_vector=True)\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_experiment(W, B, data_features, data_labels):\n",
    "    i = np.random.randint(len(data_features))\n",
    "    print(f\"Actual label: {np.argmax(data_labels[i])}\")\n",
    "    print(f\"Predicted label: {predict(W, B, data_features[i])}\")\n",
    "    \n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    m = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        m+=1\n",
    "    return cost/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE(W, B, flat_train_X, onehot_train_y) = {MSE(W, B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "random_experiment(W, B, flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c05f10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backpropogation Phase with Stochastic Gradient Descent \n",
    "We are now ready to define a custom Python ```DenseNetwork``` class which initializes the weights and bias for the network, and implements stochastic gradient descent shown below:\n",
    "\n",
    "1. For each $i = 1, \\dots, N$.\n",
    "2. Feedforward $\\mathbf{x}^{(i)}$ into the network. \n",
    "3. Compute $\\delta^{L} = \\nabla_aC\\otimes \\sigma'(\\mathbf{z}^{L})$.\n",
    "4. For $\\ell = L-1, \\dots, 1$, compute $\\delta^{\\ell} = \\big ( (\\mathbf{w}^{\\ell + 1})^{T} \\delta^{\\ell + 1} \\Big )\\otimes \\sigma'(\\mathbf{z}^{\\ell})$.\n",
    "5. For $\\ell = L, L-1, \\dots, 1$, \n",
    "\n",
    "$$\n",
    "w^{\\ell} \\leftarrow w^{\\ell} - \\alpha \\delta^{\\ell}(\\mathbf{a}^{\\ell-1})^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{\\ell} \\leftarrow b^{\\ell} - \\alpha \\delta^{\\ell}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e717c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers = self.layers)\n",
    "\n",
    "    def train(self, X_train, y_train, alpha = 0.046, epochs = 4):\n",
    "        # Print the initial mean squared error\n",
    "        self.errors_ = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors_[0]}\")\n",
    "\n",
    "        # Find your sample size\n",
    "        sample_size = len(X_train)\n",
    "\n",
    "        # Find the number of non-input layers.\n",
    "        L = len(self.layers) - 1\n",
    "\n",
    "        # For each epoch perform stochastic gradient descent. \n",
    "        for k in range(epochs):\n",
    "            # Loop over each (xi, yi) training pair of data.\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                # Compute the output error \n",
    "                output_error = (A[L] - yi)*d_sigmoid(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function \n",
    "                # is non-inclusive. \n",
    "                for i in range(L-1, 0, -1):\n",
    "                    # Compute the node errors at each hidden layer\n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*d_sigmoid(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient \n",
    "                # descent. \n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the user the cost over all training examples\n",
    "            self.errors_.append(MSE(self.W, self.B, X_train, y_train))   \n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors_[-1]}\")\n",
    "    \n",
    "\n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network with 784 input nodes, two hidden layers with 60 nodes each \n",
    "# and a output layer with 10 nodes. \n",
    "net = DenseNetwork(layers = [784, 120, 145, 120, 10])\n",
    "\n",
    "# Check the mean squared error before training \n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "# Make a random prediction before training\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525924cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddae3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935939d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a7a1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification Error\n",
    "Let us now calculate the classification percentage on the testing data for our trained dense neural network. Recall that this is simply the number of correct labels divided by the total number of data points:\n",
    "\n",
    "This can be done by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([int(net.predict(x) == y) for x, y in zip(flat_test_X, test_y)])/len(onehot_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4604b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) =mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1076a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f428cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc040175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
